
# Publications 
(Full list of  [<i class="fas fa-fw fa-graduation-cap"></i>publications](https://scholar.google.com/citations?hl=zh-CN&user=yDoybB0AAAAJ))


<style>
.box {
  display: inline-block;
  background-color: lightgray;
}

.blue-text {
  color: blue;
}
</style>

<style>
  .equal {
    font-size: 20px;
  }
</style>
<style>
  .me {
    color: blue;  
    font-weight: bold;  
  }
</style>

<style>
  .conf {
    color: brown;  
    font-weight: bold;  
  }
</style>

<blockquote style="font-size: 1em; color: blue; background-color: #f0f8ff; padding: 10px;">Journal Articles
</blockquote>

<ul style="list-style-position: outside; padding-left: 1.5em; text-indent: -1.5em;">
<li>
 ðŸ”¥ <span class="conf">[2025 arXiv]</span> <a href="https://arxiv.org/abs/2502.08921">Detecting Malicious Concepts Without Image Generation in AIGC</a> <br>
<span class="me">Kun Xu</span>, Yushu Zhang<span class="equal">*</span>, <a href="https://shurenqi.github.io/">Shuren Qi</a>, <a href="https://daizigege.github.io/">Tao Wang</a>, Wenying Wen, Yuming Fang. <br>
arXiv, [<a href="https://github.com/xukun12138/ConceptQuickLook">code</a>]
</li>


<li>
 ðŸ“‘ <span class="conf">[2025 JCES]</span> <a href="https://xukun12138.github.io/">A Facial Manipulation Adversarial Defense Approach for Image Post-Processing</a> <br>
<span class="me">Kun Xu</span>, Shuren Qi, Yushu Zhang<span class="equal">*</span>, Wenying Wen, Hua Zhang. <br>
Computer Engineering & Science, [<a href="https://github.com/xukun12138">code</a>]
</li>


<li>
 ðŸ“‘ <span class="conf">[2023 MTAP]</span> <a href="https://link.springer.com/article/10.1007/s11042-023-14626-4">Facial depth forgery detection based on image gradient</a> <br>
<span class="me">Kun Xu</span>, Gaoming Yang<span class="equal">*</span>, Xianjin Fang, Ji Zhang. <br>
Multimedia Tools and Applications,  [<a href="https://github.com/xukun12138">code</a>]
</li>


<li>
 ðŸ“‘  <span class="conf">[2023 TVCJ]</span> <a href="https://link.springer.com/article/10.1007/s00371-022-02683-z">Video face forgery detection via facial motion-assisted capturing dense optical flow truncation]</a> <br>
Gaoming Yang, <span class="me">Kun Xu</span><span class="equal">*</span>, Xianjin Fang, Ji Zhang. <br>
The Visual Computer,  [<a href="https://github.com/xukun12138">code</a>]
</li>

</ul>








<!--<blockquote style="font-size: 1em; color: blue; background-color: #f0f8ff; padding: 10px;">
Preprint.
</blockquote> -->
<!-- Preprint -->
<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">preprint</div><img src='images/advml.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Make Privacy Renewable! Generating Privacy-Preserving Faces Supporting Cancelable Biometric Recognition](https://dl.acm.org/doi/abs/10.1145/3664647.3680704) \\
   <span class="me">Tao Wang</span>, Yushu Zhang<span class="equal">*</span>, Xiangli Xiao, Lin Yuan, Zhihua Xia, Jian Weng  \[[code](https://github.com/daizigege/CanFG)\]
- Despite a decade of research, progress in securing ML models against adversarial threats remains slow, hampered by non-rigorous evaluations even in simple cases. The shift to studying LLMs introduces problems that are less defined, harder to solve, and tougher to evaluate. Without addressing these challenges, we caution that another decade of adversarial ML research may yield minimal meaningful progress.
</div>
</div>


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">preprint</div><img src='images/attack.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not Robust](https://arxiv.org/abs/2411.14834) \\
<span class="me">Jie Zhang</span>, Christian Schlarmann, Kristina NikoliÄ‡, Nicholas Carlini, Francesco Croce, Matthias Hein, Florian TramÃ¨r. \[[code](https://github.com/zj-jayzhang/attack_ens)\]

- We looked into "Ensemble Everything Everywhere", an adversarial examples defense that caused some excitement. Yet again, this serves as another example highlighting the importance of **rigorous evaluation**. 

</div>
</div> -->




<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">preprint</div><img src='images/blind_mia.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Seeing is not Believing: An Identity Hider for Human Vision Privacy Protection](https://arxiv.org/abs/2307.00481) \\
Debeshee Das, <span class="me">Jie Zhang</span>, Florian TramÃ¨r. \[[code](https://github.com/ethz-spylab/Blind-MIA)\]

- Unfortunately, we find that evaluations of MI attacks for foundation models are **flawed**, because
they sample members and non-members from different distributions. We find 8 flawed MI evaluation
datasets, existing evaluations thus tell us nothing about membership leakage of a foundation modelâ€™s training data.

</div>
</div>-->
